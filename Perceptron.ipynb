{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbjUcw92Vfi798z97EC3dq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PabloPolMartin/MachineLearning/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Single Neuron Model **Perceptron**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lZQVUbxQVwQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pablo Iván Martin Enríquez**"
      ],
      "metadata": {
        "id": "RZmMkR42bzkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intuition behind the Perceptron algorithm is to mimic the functioning of a neuron in the brain. The idea is that an artificial neuron can take multiple weighted inputs, sum them up, and produce a binary output based on whether the sum crosses a threshold. This resembles how biological neurons can either fire or not based on the information they receive. During training, the algorithm adjusts the weights of inputs so that the neuron can learn to perform simple binary classifications, such as separating two data classes in a two-dimensional space."
      ],
      "metadata": {
        "id": "-4CN_Ce5Wk6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Inputs:** Each input is multiplied by the weight associated with the synapse connecting the input to the current neuron.\n",
        "\n",
        "**Activation Function:** Activation function of a node in an artificial neural network is a function that calculates the output of the node (based on its inputs and the weights on individual inputs).\n",
        "\n",
        "Initialization: The initial weights and bias are set to zero by the initialize_with_zeros function. These parameters will be updated during training.\n",
        "\n",
        "Optimization: The optimize function performs the optimization using gradient descent. It iterates for a specified number of iterations (given as num_iterations) to minimize the cost function.\n",
        "\n",
        "Prediction: The predict function is used to make predictions. It computes the probability of each input belonging to the positive class using the learned parameters (w and b) and applies a threshold (usually 0.5) to classify the examples.\n",
        "\n",
        "Accuracy Calculation: The accuracy is calculated for both the training and test datasets. It measures how well the model performs on the given data. The accuracy is calculated as the percentage of correct predictions.\n",
        "\n",
        "Output and Results: The training accuracy and test accuracy are printed to the console to inform the user about how well the model performed. These values give an indication of the model's ability to correctly classify data."
      ],
      "metadata": {
        "id": "vpKQ7mc0VJiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudocode**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EOWo9dtnVZAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here's a pseudocode representation of the code:\n",
        "\n",
        "```plaintext\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "Both images and labels are npy's found online, this are perfect for the perceptron.\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split\n",
        "\n",
        "# Flatten and normalize the input data\n",
        "X_train_flatten\n",
        "X_test_flatten\n",
        "\n",
        "# Initialize parameters\n",
        "w, b = InitializeWithZeros(X_train_flatten.shape[0])\n",
        "\n",
        "# Define the sigmoid function\n",
        "The sigmoid function in logistic regression is used to map the linear combination of inputs and parameters to a probability in the range.\n",
        "function sigmoid(z):\n",
        "    return 1 / (1 + exp(-z))\n",
        "\n",
        "# Forward propagation\n",
        "function forward_propagation(w, b, X):\n",
        "    Z = w.T * X + b\n",
        "    A = sigmoid(Z)\n",
        "    return A\n",
        "\n",
        "# Compute the cost and gradients\n",
        "function propagate(w, b, X, Y):\n",
        "    m = X.shape[1]\n",
        "    A = forward_propagation(w, b, X)\n",
        "    cost = ComputeCost(Y, A)\n",
        "    dw, db = ComputeGradients(X, A, Y)\n",
        "    return cost, dw, db\n",
        "\n",
        "# Optimization using gradient descent\n",
        "function optimize(w, b, X, Y, num_iterations, learning_rate)\n",
        "\n",
        "# Make predictions\n",
        "function predict(w, b, X):\n",
        "    A = forward_propagation(w, b, X)\n",
        "    Y_prediction = Threshold(A)\n",
        "    return Y_prediction\n",
        "\n",
        "# Model training and evaluation\n",
        "function model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate):\n",
        "\n",
        "1.   Initialize model parameters (w and b) with zeros.\n",
        "\n",
        "2.   Use gradient descent to update the parameters (w and b) by minimizing the cost function.\n",
        "\n",
        "3.   Compute predictions for both the training and test datasets.\n",
        "\n",
        "4.   Calculate the accuracy of the model by comparing the predictions to the actual labels.\n",
        "\n",
        "Print the training and test accuracy to assess the model's performance.\n",
        "\n",
        "# Main program\n",
        "This line calls the model function, which is the primary part of the program where the logistic regression model is trained and evaluated.\n",
        "logistic_regression_model = model(X_train_flatten, y_train, X_test_flatten, y_test, num_iterations=10000, learning_rate=0.01)\n",
        "\n",
        "This gives us the results from the training and the accuracy.\n",
        "```"
      ],
      "metadata": {
        "id": "3977KG8fW4yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "XfndViznVeJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "EgoOGu0bVrZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "images = np.load(\"images.npy\")\n",
        "labels = np.load(\"labels.npy\")\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=42)\n",
        "\n",
        "# Flatten the input data and scale it\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], -1).T / 255.0\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], -1).T / 255.0\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    w = np.zeros((dim, 1), dtype=\"float64\")\n",
        "    b = 0.0\n",
        "    return w, b\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    m = X.shape[1]\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    cost = -(np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))) / m\n",
        "\n",
        "    dw = np.dot(X, (A - Y).T) / m\n",
        "    db = np.sum(A - Y) / m\n",
        "\n",
        "    cost = np.squeeze(np.array(cost))\n",
        "    grads = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "    return grads, cost\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009):\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
        "\n",
        "    params = {\"w\": w, \"b\": b}\n",
        "    grads = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "    return params, grads, costs\n",
        "\n",
        "def predict(w, b, X):\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "\n",
        "    return Y_prediction\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5):\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    params, _, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
        "    w = params[\"w\"]\n",
        "    b = params[\"b\"]\n",
        "\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n",
        "    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
        "\n",
        "    print(\"Train accuracy: {} %\".format(train_accuracy))\n",
        "    print(\"Test accuracy: {} %\".format(test_accuracy))\n",
        "\n",
        "    return {\n",
        "        \"costs\": costs,\n",
        "        \"Y_prediction_test\": Y_prediction_test,\n",
        "        \"Y_prediction_train\": Y_prediction_train,\n",
        "        \"w\": w,\n",
        "        \"b\": b,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"num_iterations\": num_iterations,\n",
        "    }"
      ],
      "metadata": {
        "id": "Hu386AA6WPMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model = model(X_train_flatten, y_train, X_test_flatten, y_test, num_iterations=10000, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac9jHVjJWno9",
        "outputId": "d9dc9aed-583a-4c39-efd6-cb8b1aeb304a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.668800\n",
            "Cost after iteration 200: 0.663867\n",
            "Cost after iteration 300: 0.661123\n",
            "Cost after iteration 400: 0.659168\n",
            "Cost after iteration 500: 0.657594\n",
            "Cost after iteration 600: 0.656243\n",
            "Cost after iteration 700: 0.655038\n",
            "Cost after iteration 800: 0.653940\n",
            "Cost after iteration 900: 0.652923\n",
            "Cost after iteration 1000: 0.651972\n",
            "Cost after iteration 1100: 0.651075\n",
            "Cost after iteration 1200: 0.650224\n",
            "Cost after iteration 1300: 0.649415\n",
            "Cost after iteration 1400: 0.648642\n",
            "Cost after iteration 1500: 0.647901\n",
            "Cost after iteration 1600: 0.647190\n",
            "Cost after iteration 1700: 0.646506\n",
            "Cost after iteration 1800: 0.645848\n",
            "Cost after iteration 1900: 0.645212\n",
            "Cost after iteration 2000: 0.644598\n",
            "Cost after iteration 2100: 0.644005\n",
            "Cost after iteration 2200: 0.643430\n",
            "Cost after iteration 2300: 0.642872\n",
            "Cost after iteration 2400: 0.642332\n",
            "Cost after iteration 2500: 0.641807\n",
            "Cost after iteration 2600: 0.641297\n",
            "Cost after iteration 2700: 0.640801\n",
            "Cost after iteration 2800: 0.640318\n",
            "Cost after iteration 2900: 0.639847\n",
            "Cost after iteration 3000: 0.639389\n",
            "Cost after iteration 3100: 0.638942\n",
            "Cost after iteration 3200: 0.638505\n",
            "Cost after iteration 3300: 0.638079\n",
            "Cost after iteration 3400: 0.637663\n",
            "Cost after iteration 3500: 0.637256\n",
            "Cost after iteration 3600: 0.636858\n",
            "Cost after iteration 3700: 0.636469\n",
            "Cost after iteration 3800: 0.636088\n",
            "Cost after iteration 3900: 0.635715\n",
            "Cost after iteration 4000: 0.635349\n",
            "Cost after iteration 4100: 0.634991\n",
            "Cost after iteration 4200: 0.634639\n",
            "Cost after iteration 4300: 0.634295\n",
            "Cost after iteration 4400: 0.633956\n",
            "Cost after iteration 4500: 0.633624\n",
            "Cost after iteration 4600: 0.633298\n",
            "Cost after iteration 4700: 0.632978\n",
            "Cost after iteration 4800: 0.632664\n",
            "Cost after iteration 4900: 0.632354\n",
            "Cost after iteration 5000: 0.632050\n",
            "Cost after iteration 5100: 0.631751\n",
            "Cost after iteration 5200: 0.631457\n",
            "Cost after iteration 5300: 0.631168\n",
            "Cost after iteration 5400: 0.630883\n",
            "Cost after iteration 5500: 0.630602\n",
            "Cost after iteration 5600: 0.630326\n",
            "Cost after iteration 5700: 0.630054\n",
            "Cost after iteration 5800: 0.629786\n",
            "Cost after iteration 5900: 0.629522\n",
            "Cost after iteration 6000: 0.629262\n",
            "Cost after iteration 6100: 0.629005\n",
            "Cost after iteration 6200: 0.628752\n",
            "Cost after iteration 6300: 0.628503\n",
            "Cost after iteration 6400: 0.628256\n",
            "Cost after iteration 6500: 0.628013\n",
            "Cost after iteration 6600: 0.627774\n",
            "Cost after iteration 6700: 0.627537\n",
            "Cost after iteration 6800: 0.627304\n",
            "Cost after iteration 6900: 0.627073\n",
            "Cost after iteration 7000: 0.626845\n",
            "Cost after iteration 7100: 0.626620\n",
            "Cost after iteration 7200: 0.626398\n",
            "Cost after iteration 7300: 0.626179\n",
            "Cost after iteration 7400: 0.625962\n",
            "Cost after iteration 7500: 0.625747\n",
            "Cost after iteration 7600: 0.625535\n",
            "Cost after iteration 7700: 0.625326\n",
            "Cost after iteration 7800: 0.625119\n",
            "Cost after iteration 7900: 0.624914\n",
            "Cost after iteration 8000: 0.624711\n",
            "Cost after iteration 8100: 0.624511\n",
            "Cost after iteration 8200: 0.624313\n",
            "Cost after iteration 8300: 0.624116\n",
            "Cost after iteration 8400: 0.623922\n",
            "Cost after iteration 8500: 0.623730\n",
            "Cost after iteration 8600: 0.623540\n",
            "Cost after iteration 8700: 0.623352\n",
            "Cost after iteration 8800: 0.623166\n",
            "Cost after iteration 8900: 0.622982\n",
            "Cost after iteration 9000: 0.622799\n",
            "Cost after iteration 9100: 0.622618\n",
            "Cost after iteration 9200: 0.622439\n",
            "Cost after iteration 9300: 0.622262\n",
            "Cost after iteration 9400: 0.622086\n",
            "Cost after iteration 9500: 0.621912\n",
            "Cost after iteration 9600: 0.621740\n",
            "Cost after iteration 9700: 0.621569\n",
            "Cost after iteration 9800: 0.621399\n",
            "Cost after iteration 9900: 0.621232\n",
            "Train accuracy: 66.125 %\n",
            "Test accuracy: 58.85 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html Perceptron.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9a5z9M4bU3T",
        "outputId": "00be0013-d5bf-4291-fdf4-c1bb71ff87b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Perceptron.ipynb to html\n",
            "[NbConvertApp] Writing 605837 bytes to Perceptron.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pablo Iván Martin Enríquez"
      ],
      "metadata": {
        "id": "5-UwX7bnbugj"
      }
    }
  ]
}